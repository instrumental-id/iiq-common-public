= IDW IIQ Common Data Exporter

IIQCommon ships with a replacement for SailPoint's various data export utilities, called `IDW Data Export`. The task writes Identity, Link, and Role information to a set of flattened tables starting with `de_` in a specified database.

Unlike the legacy "Data Export" task, this task robustly handles virtually every combination of Identity and Link attributes. Unlike the newer "Data Export", it does not require handling export events via an external message queue.

The task implementation is `com.identityworksllc.iiq.common.task.export.IDWDataExporter`. Its partition worker implementations are in the same task.

== Target database

You must set up the target database by setting the appropriate parameters in the TaskDefinition. These are essentially identical to the parameters defined on a JDBC application:

* driver
* url
* username
* password

The password must be encrypted using IIQ standard encryption.

=== Included DDL scripts

You will need to run the supplied Data Exporter DDL scripts on the target database. These will install several tables and corresponding indexes, all starting with _de_ (for Data Export).

Find the version corresponding to your database, such as _identity_link_export.oracle_ and run it in the target schema.

NOTE: You do not need to use one of the IIQ schemas, such as _identityiqPlugin_ for this, but you certainly can. Our customers typically do, and then use external BI processes to extract the desired data to a warehouse for processing.

== Specifying targets

You may specify a set of identities and a set of links to export by listing IIQ search filter strings in the TaskDefinition. Each filter will run in a separate partition, so for efficiency, it's best to break down your set into as many partitions as possible.

Identity partitions are specified in `identityFilters`. Each filter will launch one partition, which will export the identities matched by that filter. Identity partitions will export identity attributes, role assignments, and role detections.

Link partitions are sub-divided with _two_ filter lists: the required `linkFilters` and the optional `linkFilters2`. These will be combined like an M-by-N matrix. If your `linkFilters2` contains three elements, you end up with the following Link export partitions:

* linkFilters[0] AND linkFilters2[0]
* linkFilters[0] AND linkFilters2[1]
* linkFilters[0] AND linkFilters2[2]
* linkFilters[1] AND linkFilters2[0]
* linkFilters[1] AND linkFilters2[1]

And so forth.

Each partition will keep a single connection to the target database open, so ensure that your network can support such a long-lived connection.

=== Delete and add

For each identity or link, a single transaction will be used to delete all existing rows for that entity and then insert a fresh copy of the data, in that order. This should prevent any external queries from seeing partially formed data. It will also prevent unique indexes from being violated.

== Request definition

This task creates partition requests that are executed by the _SailPointWorkerExecutor_ from iiq-common-public. An appropriate RequestDefinition must be imported, such as the _IDW Worker Executor_ included with IIQCommon.

If you use a differently named RequestDefinition, specify its name in your TaskDefinition under the key `requestDefinitionName`.

If you don't specify, the default name _IDW Worker Executor_ will be used.

== Configuration

The task also requires a separate `Configuration` object, which is specified by name in the TaskDefinition, under key `configurationName`. This configuration includes global settings, which will be useful if you would like to export various items to different places, using different TaskDefinitions.

|===
|Entry |Type |Purpose

|excludeRoles
|List
|A list of role names to exclude from export. These are case sensitive.

|excludeRoleTypes
|List
|A list of role _types_ (not names) to exclude from the export. For example, you may want to export business roles and another custom type of role, but not IT roles.

|excludeIdentityColumns
|List
|A list of Identity columns to exclude from export, either because they are sensitive or irrelevant.

|excludeLinkColumns
|Map
|A map of Link columns, sorted by Application, to exclude from export. See <<Excluding Link columns>>.

|linkBatchSize
|integer
|If set, alters the JDBC batch size for committing link and link attribute export data. The default if not set is 50. Lowering this value can help with debugging.
|===

=== Excluding Link columns

Using the Configuration (_not_ the TaskDefinition), Link columns can be excluded at three levels: globally, per connector type (e.g., all JDBC applications), or per Application. Application names can be static strings or regular expressions. If regular expressions are used, applications can match more than one list of fields.

These are all specified as lists under the `excludeLinkColumns` entry.

The combined set of all fields from all relevant lists will be excluded from export.

[source,xml]
----
<entry name="excludeLinkColumns">
  <value>
    <Map>
      <entry name="global">
        <value><List>
          <String>ssn</String>
          <String>password</String>
          <String>globalSecretField</String>
        </List></value>
      </entry>
      <entry name="connector:Active Directory - Direct">
        <value><List>
          <String>password</String>
          <String>userAccountControl</String>
        </List></value>
      </entry>
      <entry name="regex:AD-.*">
        <value><List>
          <String>extensionAttribute10</String>
        </List></value>
      </entry>
    </Map>
  </value>
</entry>
----

If your AD application is called `AD-Milwaukee`, the export will exclude all five listed fields: ssn, password, globalSecretField, userAccountControl, and extensionAttribute10. All other fields will be included.

== Incremental exports

The task runs in an incremental mode by default. The incremental cutoff date is calculated _per partition_, rather than for the entire task. This allows you to resume the task if some partitions fail or add new partitions without having to repopulate the entire table.

The first execution will always be a full export. Subsequent executions will use data in the custom table `DE_RUNS` to determine which items need to be exported. The `DE_RUNS` table contains the most recent completion date for each partition. Only items created or modified after the last run date for that partition will be exported.

Rows in `DE_RUNS` are not added or updated until the partition successfully finishes. If a partition fails, its entire run must be repeated on the next export, so the previous last run timestamp will be retained.
